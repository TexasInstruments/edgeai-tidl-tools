{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Model Compilation and Inference using Tensorflow lite runtime\n",
    "\n",
    "In this example notebook, we describe how to take a pre-trained classification model and compile it using ***TF Lite runtime*** to generate deployable artifacts that can be deployed on the target using the ***TF Lite*** interface. \n",
    " \n",
    " - Pre-trained model: `mobilenetv1` model trained on ***ImageNet*** dataset using ***Tensorflow***  \n",
    " \n",
    "In particular, we will show how to\n",
    "- compile the model (during heterogenous model compilation, layers that are supported will be offloaded to the`TI-DSP` and artifacts needed for inference are generated)\n",
    "- enable debug logs\n",
    "- use deny-layer compilation option to isolate possible problematic layers and create additional model subgraphs\n",
    "- use the generated subgraphs artifacts for inference\n",
    "- perform input preprocessing and output postprocessing\n",
    "\n",
    "    \n",
    "## Tensorflow Lite Runtime based work flow\n",
    "\n",
    "The diagram below describes the steps for Tensorflow Lite Runtime based work flow. \n",
    "\n",
    "Note:\n",
    " - The user needs to compile models(sub-graph creation and quantization) on a PC to generate model artifacts.\n",
    " - The generated artifacts can then be used to run inference on the target.\n",
    "\n",
    "<img src=docs/images/osrt_user_workflow.png width=\"400\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tflite_runtime.interpreter as tflite\n",
    "import shutil \n",
    "from pathlib import Path\n",
    "from IPython.display import Markdown as md\n",
    "# import functions from local scripts\n",
    "from scripts.utils import imagenet_class_to_name, download_model\n",
    "from scripts.utils import loggerWriter\n",
    "from scripts.utils import get_svg_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the model to evaluate and images to use for fixed-point calibration\n",
    "We will set the model file to be used. If this is recognized as a TI model, we will download it from our zoo. For custom models, users will need to ensure that file is present on the local filesystem\n",
    "\n",
    "A set of calibration images are used to find an appropriate quantization so the floating point model can run on a fixed-point accelerator. For a custom-trained model, it is best to use representative data from the data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'custom-artifacts/tflite/mobilenetv1'\n",
    "tflite_model_path = 'models/public/tflite/mobilenet_v1_1.0_224.tflite'\n",
    "download_model(tflite_model_path)\n",
    "\n",
    "# For highly application-specific models, it is recommended to use representative data from your dataset\n",
    "# calibration images are used for post training quantization \n",
    "calib_images = [\n",
    "'sample-images/elephant.bmp',\n",
    "'sample-images/bus.bmp',\n",
    "'sample-images/bicycle.bmp',\n",
    "'sample-images/zebra.bmp',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define utility function to preprocess input images\n",
    "Below, we define a utility function to preprocess images for `mobilenetv1`. This function takes a path as input, loads the image and preprocesses it for generic ***TFLite*** inference. The steps are as follows: \n",
    "\n",
    " 1. load image\n",
    " 2. convert BGR image to RGB\n",
    " 3. scale image so that the short edge is 256 pixels\n",
    " 4. center-crop image to 224x224 pixels\n",
    " 5. apply per-channel pixel scaling and mean subtraction\n",
    "\n",
    "\n",
    "- Note: If you are using a custom model or a model that was trained using a different framework, please remember to define your own utility function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image_path):\n",
    "    # read the image using openCV\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    # convert to RGB\n",
    "    img = img[:,:,::-1]\n",
    "    \n",
    "    # Most of the tflite models are trained using\n",
    "    # 224x224 images. The general rule of thumb\n",
    "    # is to scale the input image while preserving\n",
    "    # the original aspect ratio so that the\n",
    "    # short edge is 256 pixels, and then\n",
    "    # center-crop the scaled image to 224x224\n",
    "    orig_height, orig_width, _ = img.shape\n",
    "    short_edge = min(img.shape[:2])\n",
    "    new_height = (orig_height * 256) // short_edge\n",
    "    new_width = (orig_width * 256) // short_edge\n",
    "    img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "    startx = new_width//2 - (224//2)\n",
    "    starty = new_height//2 - (224//2)\n",
    "    img = img[starty:starty+224,startx:startx+224]\n",
    "    \n",
    "    # apply scaling and mean subtraction.\n",
    "    # if your model is built with an input\n",
    "    # normalization layer, then you might\n",
    "    # need to skip this\n",
    "    img = img.astype('float32')\n",
    "    # mean and scale are dependent on training. The same values used to preprocess while training must be used here\n",
    "    for mean, scale, ch in zip([128, 128, 128], [0.0078125, 0.0078125, 0.0078125], range(img.shape[2])):\n",
    "            img[:,:,ch] = ((img.astype('float32')[:,:,ch] - mean) * scale)\n",
    "    img = np.expand_dims(img,axis=0)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model\n",
    "In this step, we create TFLite runtime with `tidl_model_import_tflite` delegate library to generate artifacts that offload supported portion of the DL model to the TI DSP.\n",
    " - `tidl_delegate` is created with the options below to calibrate the model for 8-bit fixed point inference\n",
    "   \n",
    "    * **tidl_tools_path** - os.getenv('TIDL_TOOLS_PATH'), path to `TIDL` compilation tools \n",
    "    * **artifacts_folder** - folder where all the compilation artifacts needed for inference are stored \n",
    "    * **tensor_bits** - 8 or 16, is the number of bits to be used for quantization \n",
    "    * **accuracy_level** - 1 or 0, the desired accuracy with quantized model\n",
    "    * **advanced_options:calibration_frames** - number of images to be used for calibration\n",
    "    * **advanced_options:calibration_iterations** - number of iterations for advanced calibration\n",
    "    * **debug_level** - 0 -> no debug, 1 -> rt debug prints, >=2 -> increasing levels of debug and trace dump\n",
    "    * **deny_list** force disable offload of a particular operator to TIDL. \n",
    "    \n",
    "- Note: The path to `TIDL` compilation tools and `aarch64` `GCC` compiler is required for model compilation, both of which are accessed by this notebook using predefined environment variables `TIDL_TOOLS_PATH` and `ARM64_GCC_PATH`. The example usage of both the variables is demonstrated in the cell below. \n",
    "- Please refer to TIDL user guide and the edgeai-tidl-tools repository documentation for further advanced options.\n",
    "\n",
    "### Layers debug (optional - In case of debugging)\n",
    "Debug_level 1 gives layer information and warnings/errors which could be useful during debug. User's can see logs from compilation inside a giving path to \"loggerWriter\" helper function.\n",
    "\n",
    "Another technique is to use deny_list to exclude layers from running on TIDL and create additional subgraphs, in order to aisolate issues.\n",
    "\n",
    "### Compilation knobs  (optional - In case of debugging accuracy)\n",
    "if a model accuracy at 8bits is not good, user's can try compiling same model at 16 bits with accuracy level of 1. This will reduce the performance, but it will give users a good accuracy bar.\n",
    "As a second step, user can try to increase 8 bits accuracy by increasing the number of calibration frames and iterations, in order to get closer to 16 bits + accuracy level of 1 results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = Path(\"logs\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# debug level -- use 1 or 2 for increased verbosity in the error messages. \n",
    "# See log files to view all printed messages\n",
    "debug_level=0 \n",
    "\n",
    "#compilation options - knobs to tweak \n",
    "num_bits =8\n",
    "accuracy =1\n",
    "\n",
    "# stdout and stderr saved to a *.log file.  \n",
    "with loggerWriter(\"logs/custon-model-tfl\"):\n",
    "    \n",
    "# model compilation options\n",
    "    compile_options = {\n",
    "        'tidl_tools_path' : os.environ['TIDL_TOOLS_PATH'],\n",
    "        'artifacts_folder' : output_dir,\n",
    "        'tensor_bits' : num_bits,\n",
    "        'accuracy_level' : accuracy,\n",
    "        'debug_level' : debug_level,\n",
    "        'advanced_options:calibration_frames' : len(calib_images),\n",
    "        'advanced_options:calibration_iterations' : 3,\n",
    "        'advanced_options:add_data_convert_ops' : 1,\n",
    "        #'deny_list' : 1, #For details of TFLite builtin ops please refer: https://github.com/tensorflow/tensorflow/blob/r2.3/tensorflow/lite/builtin_ops.h\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> Please note 'deny_list' is used in above cell as an example and it can be deleted as \"AveragePool2d\" is a supported layers\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the output dir if not preset\n",
    "# clear the directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for root, dirs, files in os.walk(output_dir, topdown=False):\n",
    "    [os.remove(os.path.join(root, f)) for f in files]\n",
    "    [os.rmdir(os.path.join(root, d)) for d in dirs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the compilation itself\n",
    "# Load the TIDL delegate for model import/compilation in TFLite\n",
    "# Also set the compile_options defined above to set parameters for the compilation process\n",
    "tidl_delegate = [tflite.load_delegate(os.path.join(os.environ['TIDL_TOOLS_PATH'], 'tidl_model_import_tflite.so'), compile_options)]\n",
    "\n",
    "# Create a new runtime interpreter for your model targeting the TIDL import delegate\n",
    "# When this call runs, compilation will begin but not complete because it is waiting for calibration data\n",
    "interpreter = tflite.Interpreter(model_path=tflite_model_path, experimental_delegates=tidl_delegate)\n",
    "\n",
    "# Typical TFL API calls prior to usage\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Preprocess calibration data and pass it to the runtime. \n",
    "# Once at least 'calibration_frames' number of images are passed in, calibration can proceed\n",
    "# Model compilation can complete after all calibration data is received and processed\n",
    "for num in tqdm.trange(len(calib_images)):\n",
    "    interpreter.set_tensor(input_details[0]['index'], preprocess(calib_images[num]))\n",
    "    interpreter.invoke()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subgraphs visualization  (optional - In case of debugging models and subgraphs)\n",
    "\n",
    "TIDL processes 'subgraphs' of supported layers that can run on the acclerator. Several SVG files are provided to visualize the network as a graph.\n",
    "\n",
    "Running below cell gives links to complete graph and TIDL subgraphs visualizations. This, along with \"deny_list\" feature, explained above, offer tools for potentially checking and isolating issues in the neural network model layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph_link =get_svg_path(output_dir) \n",
    "for sg in subgraph_link:\n",
    "    hl_text = os.path.join(*Path(sg).parts[4:])\n",
    "    sg_rel = os.path.join('../', sg)\n",
    "    display(md(\"[{}]({})\".format(hl_text,sg_rel))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use compiled model for inference\n",
    "Then using ***TF Lite*** with the ***`libtidl_tfl_delegate`*** delegate library we run the model and collect benchmark data.\n",
    "\n",
    "This time we will use the ***`libtidl_tfl_delegate`*** instead of the ***`tidl_model_import_tflite`*** delegate, which was used for compiling/importing the model.\n",
    "\n",
    "The  ***`libtidl_tfl_delegate`*** will similarly accept a dictionary of options to be passed into the TIDL runtime, but this need not be as extensive as the compile_options. This time, all be need is the 'artifacts_folder' pointing to the directory where we put the TIDL compilation outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setup the TIDL TFL delegate for inference, and point to the compiled model artifacts\n",
    "tidl_delegate = [tflite.load_delegate('libtidl_tfl_delegate.so', {'artifacts_folder': output_dir})]\n",
    "# Setup the Interpreter for this model and target the tidl delegate\n",
    "interpreter = tflite.Interpreter(model_path=tflite_model_path, experimental_delegates=tidl_delegate)\n",
    "\n",
    "# Typical TFL API calls needs to complete setup of the network \n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "interpreter.set_tensor(input_details[0]['index'], preprocess('sample-images/elephant.bmp'))\n",
    "\n",
    "#Running inference several times to get an stable performance output\n",
    "for i in range(5):\n",
    "    interpreter.invoke()\n",
    "    \n",
    "res = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "# Postprocess the output to determine what was recognized in the image. For non-classification model, add your own postprocessing function\n",
    "for idx, cls in enumerate(res[0].squeeze()[1:].argsort()[-5:][::-1]):\n",
    "    print('[%d] %s' % (idx, '/'.join(imagenet_class_to_name(cls))))\n",
    "    \n",
    "# Pull TI performance measurements from the runtime\n",
    "from scripts.utils import plot_TI_performance_data, plot_TI_DDRBW_data, get_benchmark_output\n",
    "stats = interpreter.get_TI_benchmark_data()\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,5))\n",
    "plot_TI_performance_data(stats, axis=ax)\n",
    "plt.show()\n",
    "\n",
    "# Process runtime stats to get total time (tt), processing time(st), ddr read time (rb), and ddr write time (wb) for one model inference\n",
    "tt, st, rb, wb = get_benchmark_output(stats)\n",
    "print(f'Statistics : \\n Inferences Per Second   : {1000.0/tt :7.2f} fps')\n",
    "print(f' Inference Time Per Image : {tt :7.2f} ms  \\n DDR BW Per Image        : {rb+ wb : 7.2f} MB')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVM's console logs (optional - in case of inference failure)\n",
    "\n",
    "To copy console logs from EVM to TI EdgeAI Cloud user's workspace, go to: \"Help -> Troubleshooting -> EVM console log\", In TI's EdgeAI Cloud landing page.\n",
    "\n",
    "Alternatevely, from workspace, open/run evm-console-log.ipynb"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
