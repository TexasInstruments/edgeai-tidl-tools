{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification Inference for High Resolution Images - ONNX Runtime\n",
    "In this example notebook, we describe how to use a pre-trained Classification model, using high resolution images, for inference. \n",
    "   - The user can choose the model (see section titled *Choosing a Pre-Compiled Model*)\n",
    "   - The models used in this example were trained on the ***ImageNet*** dataset because it is a widely used dataset developed for training and benchmarking image classification AI models. \n",
    "   - We perform inference on a few sample images.\n",
    "   - We also describe the input preprocessing and output postprocessing steps, demonstrate how to collect various benchmarking statistics and how to visualize the data.\n",
    "   \n",
    "## Choosing a Pre-Compiled Model\n",
    "We provide a set of precompiled artifacts to use with this notebook that will appear as a drop-down list once the first code cell is executed.\n",
    "\n",
    "<img src=docs/images/drop_down.PNG width=\"400\">\n",
    "\n",
    "## Image classification\n",
    "Image classification is a popular computer vision algorithm used in applications such as, object recongnition, traffic sign recongnition and traffic light recongnition. Image classification models are also used as feature extractors for other tasks such as object detection and semantic segmentation.\n",
    "   - The image below shows classification results on few sample images.\n",
    "   - Note: in this example, we used models trained with ***ImageNet*** because it is a widely used dataset developed for training and benchmarking image classifcation AI models\n",
    "\n",
    "## ONNX Runtime based Work flow\n",
    "The diagram below describes the steps for ONNX Runtime based workflow. \n",
    "\n",
    "Note:\n",
    "- The user needs to compile models(sub-graph creation and quantization) on a PC to generate model artifacts.\n",
    "    - For this notebook we use pre-compiled models artifacts\n",
    "- The generated artifacts can then be used to run inference on the target.\n",
    "- Users can run this notebook as-is, only action required is to select a model.\n",
    "\n",
    "<img src=docs/images/onnx_work_flow_2.png width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from scripts.utils import get_eval_configs\n",
    "#grab a set of model configurations locally defined in a script\n",
    "last_artifacts_id = selected_model_id.value if \"selected_model_id\" in locals() else None\n",
    "prebuilt_configs, selected_model_id = get_eval_configs('classification','onnxrt', num_quant_bits = 8, last_artifacts_id = last_artifacts_id, model_selection='high_resolution')\n",
    "display(selected_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> High resolution models (512x512 1024x1024) are for feature extraction and they were not trained for accuracy. Please use those high resolutions models for fps/performance measurement only\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Selected Model: {selected_model_id.label}')\n",
    "config = prebuilt_configs[selected_model_id.value]\n",
    "config['session'].set_param('model_id', selected_model_id.value)\n",
    "config['session'].start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define utility function to preprocess input images\n",
    "\n",
    "Below, we define a utility function to preprocess images for the model. This function takes a path as input, loads the image and preprocesses the images as required by the model. The steps below are shown as a reference (no user action required):\n",
    "\n",
    " 1. Load image\n",
    " 2. Convert BGR image to RGB\n",
    " 3. Scale image\n",
    " 4. Apply per-channel pixel scaling and mean subtraction\n",
    " 5. Convert RGB Image to BGR. \n",
    " 6. Convert the image to NCHW format\n",
    "\n",
    "\n",
    "- The input arguments of this utility function is selected automatically by this notebook based on the model selected in the drop-down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image_path, size, mean, scale, layout, reverse_channels):\n",
    "    # Step 1 - read image\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    # Step 2 - Flip from BGR to RGB\n",
    "    img = img[:,:,::-1]\n",
    "    \n",
    "    # Step 3 -- resize to match model input dimensions \n",
    "    img = cv2.resize(img, (size, size), interpolation=cv2.INTER_CUBIC)\n",
    "     \n",
    "    # Step 4 - subtract a mean and multiply a scale to match model's expected data distributions\n",
    "    if mean is not None and scale is not None:   \n",
    "        img = img.astype('float32')\n",
    "        for mean, scale, ch in zip(mean, scale, range(img.shape[2])):\n",
    "            img[:,:,ch] = ((img.astype('float32')[:,:,ch] - mean) * scale)\n",
    "    # Step 5 - If needed, flip back to BGR\n",
    "    if reverse_channels:\n",
    "        img = img[:,:,::-1]\n",
    "        \n",
    "    # Step 6 -- Reorder tensor dimensions as NCHW (number, channel, height, width) or NHWC\n",
    "    if layout == 'NCHW':\n",
    "        img = np.expand_dims(np.transpose(img, (2,0,1)),axis=0)\n",
    "    else:\n",
    "        img = np.expand_dims(img,axis=0)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model using the stored artifacts\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning:</b> It is recommended to use the ONNX Runtime APIs in the cells below without any modifications.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as rt\n",
    "\n",
    "onnx_model_path = config['session'].get_param('model_file')\n",
    "delegate_options = {}\n",
    "so = rt.SessionOptions()\n",
    "delegate_options['artifacts_folder'] = config['session'].get_param('artifacts_folder')\n",
    "# Designate the TIDL execution provider to enable layer acceleration\n",
    "EP_list = ['TIDLExecutionProvider','CPUExecutionProvider']\n",
    "# Create the model's InferenceSession by passing the set of providers and corresponding provider options\n",
    "sess = rt.InferenceSession(onnx_model_path ,providers=EP_list, provider_options=[delegate_options, {}], sess_options=so)\n",
    "\n",
    "input_details = sess.get_inputs()\n",
    "output_details = sess.get_outputs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model for inference\n",
    "\n",
    "### Preprocessing and Inference\n",
    "\n",
    "   - You can use a portion of images provided in `/sample-images` directory to evaluate the classification inferences. In the cell below, we use a loop to preprocess the selected images, and provide them as the input to the network.\n",
    "\n",
    "### Postprocessing and Visualization\n",
    "\n",
    " - Once the inference results are available, we postpocess the results and visualize the inferred classes for each of the input images.\n",
    " - Classification models return the results as a list of `numpy.ndarray`, containing one element which is an array with `shape` = `(1,1000)` and `dtype` = `'float32'`, where each element represents the activation for a particular ***ImageNet*** class. The results from the these inferences above are postprocessed using `argsort()` to get the `TOP-5` class IDs and the corresponding names using `imagenet_class_to_name()`.\n",
    " - Then, in this notebook, we use *matplotlib* to plot the original images and the corresponding results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scripts.utils import get_preproc_props\n",
    "\n",
    "# use results from the past inferences\n",
    "images = [\n",
    "    ('sample-images/elephant.bmp', 221),\n",
    "    ('sample-images/laptop.bmp', 222),\n",
    "    ('sample-images/bus.bmp', 223),\n",
    "    ('sample-images/zebra.bmp', 224),\n",
    "]\n",
    "size, mean, scale, layout, reverse_channels = get_preproc_props(config)    \n",
    "print(f'Image size: {size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scripts.utils import imagenet_class_to_name\n",
    "\n",
    "for num in tqdm.trange(len(images)):\n",
    "    image_file, grid = images[num]\n",
    "    img = cv2.imread(image_file)[:,:,::-1]\n",
    "\n",
    "    img_in = preprocess(image_file , size, mean, scale, layout, reverse_channels) \n",
    "    if not input_details[0].type == 'tensor(float)':\n",
    "        img_in = np.uint8(img_in)\n",
    "        \n",
    "    res = list(sess.run(None, {input_details[0].name: img_in}))[0]\n",
    "\n",
    "    # Postprocessing -- get the TOP-5 class IDs by argsort()\n",
    "    # and use utility function to get names\n",
    "    output = res.squeeze()\n",
    "    classes = output.argsort()[-5:][::-1]\n",
    "    names = [imagenet_class_to_name(x)[0] for x in classes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Inference benchmarking statistics\n",
    "\n",
    " - During the model execution several benchmarking statistics such as timestamps at different checkpoints, DDR bandwidth are collected and stored. `get_TI_benchmark_data()` can be used to collect these statistics. This function returns a dictionary of `annotations` and the corresponding markers.\n",
    " - We provide the utility function plot_TI_benchmark_data to visualize these benchmark KPIs\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> The values represented by <i>Inferences Per Second</i> and <i>Inference Time Per Image</i> uses the total time taken by the inference except the time taken for copying inputs and outputs. In a performance oriented system, these operations can be bypassed by writing the data directly into shared memory and performing on-the-fly input / output normalization.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scripts.utils import plot_TI_performance_data, plot_TI_DDRBW_data, get_benchmark_output, print_soc_info\n",
    "# Pull TI performance measurements from the runtime\n",
    "stats = sess.get_TI_benchmark_data()\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,5))\n",
    "plot_TI_performance_data(stats, axis=ax)\n",
    "plt.show()\n",
    "# Process stats to get total time (tt), processing time(st), ddr read time (rb), and ddr write time (wb) for one model inference\n",
    "tt, st, rb, wb = get_benchmark_output(stats)\n",
    "\n",
    "print_soc_info()\n",
    "print(f'{selected_model_id.label} :')\n",
    "print(f' Inferences Per Second    : {1000.0/tt :7.2f} fps')\n",
    "print(f' Inference Time Per Image : {tt :7.2f} ms')\n",
    "print(f' DDR usage Per Image      : {rb+ wb : 7.2f} MB')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
